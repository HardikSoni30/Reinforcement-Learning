{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48a68c34-bb65-413d-bc40-da6b4b922e81",
   "metadata": {},
   "source": [
    "# Temporal Difference Learning\n",
    "\n",
    "The code in this notebook implements different methods for simulating and analyzing a **Random Walk** environment using **Temporal Difference (TD)** learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547cd940-435c-42f2-926e-6822d81897c5",
   "metadata": {},
   "source": [
    "### Purpose of the Code:\n",
    "1. **Random Walk Environment**: The simulation represents a random walk between two terminal states (0 and 6). The agent starts in the middle state (3) and takes random actions (either left or right). The goal is to reach the terminal state on the right (6), which provides a reward of 1, while the left terminal state (0) gives no reward.\n",
    "\n",
    "2. **Temporal Difference (TD) Learning**: \n",
    "   - The notebook employs **TD(0)**, a reinforcement learning algorithm, to estimate the state-value function $ V(s) $, which represents the expected reward when starting from a given state.\n",
    "   - It explores how well the algorithm estimates the true value function through several episodes, updating the value of each state based on the agent's experience during the episodes.\n",
    "\n",
    "3. **Policy Comparison**: The notebook also includes two different policies for action selection, although it mainly focuses on how the value function is learned by interacting with the environment. \n",
    "\n",
    "4. **Evaluation of Learning**: The success of the learning process is evaluated by comparing the estimated value function with a \"true\" value function. The performance is measured using Root Mean Square (RMS) error to quantify how close the learned values are to the true values.\n",
    "\n",
    "### Sections of the Code:\n",
    "1. **Defining the Random Walk Environment**: \n",
    "   The environment has 7 states, where the agent performs random actions to move either left or right. The agentâ€™s goal is to reach the rewarding state (state 6).\n",
    "\n",
    "2. **TD(0) Algorithm**: \n",
    "   The core of the code implements TD(0), where state values are updated incrementally after each action based on the reward and the value of the next state.\n",
    "\n",
    "3. **Evaluation**: \n",
    "   After running a set number of episodes, the learned value function is printed to observe how well the agent has learned to predict the expected rewards for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1d08437-791d-4bc9-ab1d-c64a8d431257",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T12:48:23.578788Z",
     "start_time": "2024-09-16T12:48:23.507994Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55bc2d28-4080-4ffe-92b7-229c46de7a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomWalkEnv:\n",
    "    def __init__(self):\n",
    "        self.states = list(range(7))  # States from 0 to 6\n",
    "        self.terminal_states = [0, 6]\n",
    "        self.current_state = 3  # Initial position is \"C\"\n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_state = 3  # Reset to initial state\n",
    "        return self.current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        # If already in a terminal state, return immediately\n",
    "        if self.current_state in self.terminal_states:\n",
    "            return self.current_state, 0, True\n",
    "        \n",
    "        # Move to the next state based on the action\n",
    "        # if action == 0:  # Move left\n",
    "        #     next_state = self.current_state - 1\n",
    "        # else:  # Move right\n",
    "        #     next_state = self.current_state + 1\n",
    "        next_state = self.current_state + (1 if action == 1 else -1)\n",
    "        \n",
    "        # Ensure next_state is within bounds\n",
    "        next_state = max(0, min(next_state, 6))\n",
    "        \n",
    "        # Determine reward and done status\n",
    "        reward = 1 if next_state == 6 else 0\n",
    "        done = next_state in self.terminal_states\n",
    "        \n",
    "        # Update the current state\n",
    "        self.current_state = next_state\n",
    "        \n",
    "        return self.current_state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a1bf620-aaba-42d2-aa23-04af20ab8e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(env, policy, num_steps=10):\n",
    "    state = env.reset()\n",
    "    print(\"Starting from State 3...\")\n",
    "    print(f\"{'Action':<10} {'State':<10} {'Reward':<10} {'Done':<10}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for _ in range(num_steps):\n",
    "        action = np.random.choice([0, 1], p=policy[state])\n",
    "        state, reward, done = env.step(action)\n",
    "        print(f\"{ACTION_NAMES[action]:<10} {state:<10} {reward:<10} {done:<10}\")\n",
    "        \n",
    "        if done:\n",
    "            state = env.reset()  # Reset the environment if done\n",
    "            print(\"Restarting from State 3...\")\n",
    "    \n",
    "    return state, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c00d45f-bf3d-45b6-9320-0cda3c71c260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the environment\n",
    "ACTION_NAMES = [\"LEFT\", \"RIGHT\"]\n",
    "env_rw = RandomWalkEnv()\n",
    "policy_b = np.array([[0.4, 0.6], [0.45, 0.55], [0.5, 0.5], [0.7, 0.3], [0.5, 0.5], [0.6, 0.4], [0.9, 0.1]])\n",
    "policy_pi = np.array([[0.5, 0.5]] * 7) # uniform probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8bac627-dd5c-4e0d-ae77-f3eed969c3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from State 3...\n",
      "Action     State      Reward     Done      \n",
      "----------------------------------------\n",
      "LEFT       2          0          0         \n",
      "RIGHT      3          0          0         \n",
      "RIGHT      4          0          0         \n",
      "LEFT       3          0          0         \n",
      "LEFT       2          0          0         \n",
      "LEFT       1          0          0         \n",
      "LEFT       0          0          1         \n",
      "Restarting from State 3...\n",
      "LEFT       2          0          0         \n",
      "LEFT       1          0          0         \n",
      "LEFT       0          0          1         \n",
      "Restarting from State 3...\n"
     ]
    }
   ],
   "source": [
    "state, reward, done = run_simulation(env_rw, policy_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b6ad277-a87a-49dd-b730-bb7daf8ad6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from State 3...\n",
      "Action     State      Reward     Done      \n",
      "----------------------------------------\n",
      "LEFT       2          0          0         \n",
      "RIGHT      3          0          0         \n",
      "LEFT       2          0          0         \n",
      "LEFT       1          0          0         \n",
      "LEFT       0          0          1         \n",
      "Restarting from State 3...\n",
      "LEFT       2          0          0         \n",
      "RIGHT      3          0          0         \n",
      "LEFT       2          0          0         \n",
      "LEFT       1          0          0         \n",
      "LEFT       0          0          1         \n",
      "Restarting from State 3...\n"
     ]
    }
   ],
   "source": [
    "state, reward, done = run_simulation(env_rw, policy_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ce17195-2714-4a39-a072-1548579e6819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode_with_timestep(env, policy):\n",
    "    state = env.reset()\n",
    "    episode = []\n",
    "    time_step = 0\n",
    "    \n",
    "    print(\"Starting from State 3...\")\n",
    "    print(f\"{'Time Step':<10} {'Action':<10} {'State':<10} {'Reward':<10} {'Done':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    while True:\n",
    "        action = np.random.choice([0, 1], p=policy[state])\n",
    "        next_state, reward, done = env.step(action)\n",
    "        episode.append((time_step, state, action, reward, next_state, done))\n",
    "        \n",
    "        print(f\"{time_step:<10} {ACTION_NAMES[action]:<10} {next_state:<10} {reward:<10} {done:<10}\")\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        state = next_state\n",
    "        time_step += 1\n",
    "        \n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b43d3109-f2a9-4a19-a997-c2f509ffa1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from State 3...\n",
      "Time Step  Action     State      Reward     Done      \n",
      "--------------------------------------------------\n",
      "0          RIGHT      4          0          0         \n",
      "1          LEFT       3          0          0         \n",
      "2          LEFT       2          0          0         \n",
      "3          LEFT       1          0          0         \n",
      "4          LEFT       0          0          1         \n"
     ]
    }
   ],
   "source": [
    "episode_policy_pi = run_episode_with_timestep(env_rw, policy_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92458d11-a1b0-41b6-ac61-072af6f2d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# episode_policy_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "319daa44-e120-4eb3-9a37-2a184c086bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from State 3...\n",
      "Time Step  Action     State      Reward     Done      \n",
      "--------------------------------------------------\n",
      "0          LEFT       2          0          0         \n",
      "1          LEFT       1          0          0         \n",
      "2          RIGHT      2          0          0         \n",
      "3          RIGHT      3          0          0         \n",
      "4          LEFT       2          0          0         \n",
      "5          RIGHT      3          0          0         \n",
      "6          LEFT       2          0          0         \n",
      "7          LEFT       1          0          0         \n",
      "8          RIGHT      2          0          0         \n",
      "9          RIGHT      3          0          0         \n",
      "10         LEFT       2          0          0         \n",
      "11         RIGHT      3          0          0         \n",
      "12         LEFT       2          0          0         \n",
      "13         RIGHT      3          0          0         \n",
      "14         LEFT       2          0          0         \n",
      "15         RIGHT      3          0          0         \n",
      "16         RIGHT      4          0          0         \n",
      "17         LEFT       3          0          0         \n",
      "18         LEFT       2          0          0         \n",
      "19         RIGHT      3          0          0         \n",
      "20         LEFT       2          0          0         \n",
      "21         RIGHT      3          0          0         \n",
      "22         LEFT       2          0          0         \n",
      "23         RIGHT      3          0          0         \n",
      "24         LEFT       2          0          0         \n",
      "25         LEFT       1          0          0         \n",
      "26         LEFT       0          0          1         \n"
     ]
    }
   ],
   "source": [
    "episode_policy_b = run_episode_with_timestep(env_rw, policy_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e2e1855-de72-49b2-a7b9-69a873e5d7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# episode_policy_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f606ac6-2dec-4926-9de8-d19501b469e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multiple_episodes(env, policy_pi, num_episodes=10):\n",
    "    successes = 0\n",
    "    \n",
    "    for episode_num in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        time_step = 0\n",
    "        success = False\n",
    "        \n",
    "        print(f\"Starting Episode {episode_num + 1} from State 3...\")\n",
    "        print(f\"{'Time Step':<10} {'Action':<10} {'State':<10} {'Reward':<10} {'Done':<10}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        while True:\n",
    "            action = np.random.choice([0, 1], p=policy_pi[state])\n",
    "            next_state, reward, done = env.step(action)\n",
    "            time_step += 1\n",
    "            \n",
    "            print(f\"{time_step:<10} {ACTION_NAMES[action]:<10} {next_state:<10} {reward:<10} {done:<10}\")\n",
    "            \n",
    "            if done or next_state == 6:\n",
    "                success = next_state == 6\n",
    "                break\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        if success:\n",
    "            print(\"Successfully reached State 6!\")\n",
    "            successes += 1\n",
    "        else:\n",
    "            print(\"Failed to reach State 6.\")\n",
    "        \n",
    "        print(\"=\" * 50)\n",
    "    \n",
    "    success_rate = successes / num_episodes\n",
    "    print(f\"Success Rate: {success_rate * 100:.2f}%\")\n",
    "    \n",
    "    return success_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67c5e0da-8a93-4419-bd10-8f1e2a1f0570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Episode 1 from State 3...\n",
      "Time Step  Action     State      Reward     Done      \n",
      "--------------------------------------------------\n",
      "1          RIGHT      4          0          0         \n",
      "2          RIGHT      5          0          0         \n",
      "3          RIGHT      6          1          1         \n",
      "Successfully reached State 6!\n",
      "==================================================\n",
      "Starting Episode 2 from State 3...\n",
      "Time Step  Action     State      Reward     Done      \n",
      "--------------------------------------------------\n",
      "1          RIGHT      4          0          0         \n",
      "2          RIGHT      5          0          0         \n",
      "3          RIGHT      6          1          1         \n",
      "Successfully reached State 6!\n",
      "==================================================\n",
      "Starting Episode 3 from State 3...\n",
      "Time Step  Action     State      Reward     Done      \n",
      "--------------------------------------------------\n",
      "1          RIGHT      4          0          0         \n",
      "2          LEFT       3          0          0         \n",
      "3          LEFT       2          0          0         \n",
      "4          LEFT       1          0          0         \n",
      "5          RIGHT      2          0          0         \n",
      "6          LEFT       1          0          0         \n",
      "7          RIGHT      2          0          0         \n",
      "8          LEFT       1          0          0         \n",
      "9          RIGHT      2          0          0         \n",
      "10         LEFT       1          0          0         \n",
      "11         RIGHT      2          0          0         \n",
      "12         RIGHT      3          0          0         \n",
      "13         LEFT       2          0          0         \n",
      "14         RIGHT      3          0          0         \n",
      "15         LEFT       2          0          0         \n",
      "16         RIGHT      3          0          0         \n",
      "17         LEFT       2          0          0         \n",
      "18         LEFT       1          0          0         \n",
      "19         RIGHT      2          0          0         \n",
      "20         RIGHT      3          0          0         \n",
      "21         LEFT       2          0          0         \n",
      "22         LEFT       1          0          0         \n",
      "23         RIGHT      2          0          0         \n",
      "24         LEFT       1          0          0         \n",
      "25         RIGHT      2          0          0         \n",
      "26         RIGHT      3          0          0         \n",
      "27         LEFT       2          0          0         \n",
      "28         RIGHT      3          0          0         \n",
      "29         RIGHT      4          0          0         \n",
      "30         RIGHT      5          0          0         \n",
      "31         LEFT       4          0          0         \n",
      "32         RIGHT      5          0          0         \n",
      "33         RIGHT      6          1          1         \n",
      "Successfully reached State 6!\n",
      "==================================================\n",
      "Starting Episode 4 from State 3...\n",
      "Time Step  Action     State      Reward     Done      \n",
      "--------------------------------------------------\n",
      "1          RIGHT      4          0          0         \n",
      "2          LEFT       3          0          0         \n",
      "3          RIGHT      4          0          0         \n",
      "4          LEFT       3          0          0         \n",
      "5          LEFT       2          0          0         \n",
      "6          LEFT       1          0          0         \n",
      "7          LEFT       0          0          1         \n",
      "Failed to reach State 6.\n",
      "==================================================\n",
      "Starting Episode 5 from State 3...\n",
      "Time Step  Action     State      Reward     Done      \n",
      "--------------------------------------------------\n",
      "1          LEFT       2          0          0         \n",
      "2          LEFT       1          0          0         \n",
      "3          LEFT       0          0          1         \n",
      "Failed to reach State 6.\n",
      "==================================================\n",
      "Starting Episode 6 from State 3...\n",
      "Time Step  Action     State      Reward     Done      \n",
      "--------------------------------------------------\n",
      "1          RIGHT      4          0          0         \n",
      "2          LEFT       3          0          0         \n",
      "3          LEFT       2          0          0         \n",
      "4          RIGHT      3          0          0         \n",
      "5          LEFT       2          0          0         \n",
      "6          RIGHT      3          0          0         \n",
      "7          LEFT       2          0          0         \n",
      "8          LEFT       1          0          0         \n",
      "9          LEFT       0          0          1         \n",
      "Failed to reach State 6.\n",
      "==================================================\n",
      "Starting Episode 7 from State 3...\n",
      "Time Step  Action     State      Reward     Done      \n",
      "--------------------------------------------------\n",
      "1          LEFT       2          0          0         \n",
      "2          LEFT       1          0          0         \n",
      "3          LEFT       0          0          1         \n",
      "Failed to reach State 6.\n",
      "==================================================\n",
      "Starting Episode 8 from State 3...\n",
      "Time Step  Action     State      Reward     Done      \n",
      "--------------------------------------------------\n",
      "1          RIGHT      4          0          0         \n",
      "2          LEFT       3          0          0         \n",
      "3          RIGHT      4          0          0         \n",
      "4          LEFT       3          0          0         \n",
      "5          RIGHT      4          0          0         \n",
      "6          LEFT       3          0          0         \n",
      "7          RIGHT      4          0          0         \n",
      "8          RIGHT      5          0          0         \n",
      "9          LEFT       4          0          0         \n",
      "10         RIGHT      5          0          0         \n",
      "11         LEFT       4          0          0         \n",
      "12         LEFT       3          0          0         \n",
      "13         RIGHT      4          0          0         \n",
      "14         RIGHT      5          0          0         \n",
      "15         RIGHT      6          1          1         \n",
      "Successfully reached State 6!\n",
      "==================================================\n",
      "Starting Episode 9 from State 3...\n",
      "Time Step  Action     State      Reward     Done      \n",
      "--------------------------------------------------\n",
      "1          LEFT       2          0          0         \n",
      "2          RIGHT      3          0          0         \n",
      "3          LEFT       2          0          0         \n",
      "4          LEFT       1          0          0         \n",
      "5          RIGHT      2          0          0         \n",
      "6          RIGHT      3          0          0         \n",
      "7          RIGHT      4          0          0         \n",
      "8          LEFT       3          0          0         \n",
      "9          LEFT       2          0          0         \n",
      "10         LEFT       1          0          0         \n",
      "11         LEFT       0          0          1         \n",
      "Failed to reach State 6.\n",
      "==================================================\n",
      "Starting Episode 10 from State 3...\n",
      "Time Step  Action     State      Reward     Done      \n",
      "--------------------------------------------------\n",
      "1          RIGHT      4          0          0         \n",
      "2          LEFT       3          0          0         \n",
      "3          RIGHT      4          0          0         \n",
      "4          RIGHT      5          0          0         \n",
      "5          RIGHT      6          1          1         \n",
      "Successfully reached State 6!\n",
      "==================================================\n",
      "Success Rate: 50.00%\n"
     ]
    }
   ],
   "source": [
    "success_rate_policy_pi = run_multiple_episodes(env_rw, policy_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47f188c0-4696-4a3c-8d85-74dc584645d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Episode 1 from State 3...\n",
      "Time Step  Action     State      Reward     Done      \n",
      "--------------------------------------------------\n",
      "1          RIGHT      4          0          0         \n",
      "2          LEFT       3          0          0         \n",
      "3          LEFT       2          0          0         \n",
      "4          LEFT       1          0          0         \n",
      "5          LEFT       0          0          1         \n",
      "Failed to reach State 6.\n",
      "==================================================\n",
      "Starting Episode 2 from State 3...\n",
      "Time Step  Action     State      Reward     Done      \n",
      "--------------------------------------------------\n",
      "1          LEFT       2          0          0         \n",
      "2          LEFT       1          0          0         \n",
      "3          LEFT       0          0          1         \n",
      "Failed to reach State 6.\n",
      "==================================================\n",
      "Starting Episode 3 from State 3...\n",
      "Time Step  Action     State      Reward     Done      \n",
      "--------------------------------------------------\n",
      "1          RIGHT      4          0          0         \n",
      "2          RIGHT      5          0          0         \n",
      "3          LEFT       4          0          0         \n",
      "4          RIGHT      5          0          0         \n",
      "5          LEFT       4          0          0         \n",
      "6          RIGHT      5          0          0         \n",
      "7          LEFT       4          0          0         \n",
      "8          RIGHT      5          0          0         \n",
      "9          LEFT       4          0          0         \n",
      "10         RIGHT      5          0          0         \n",
      "11         LEFT       4          0          0         \n",
      "12         RIGHT      5          0          0         \n",
      "13         RIGHT      6          1          1         \n",
      "Successfully reached State 6!\n",
      "==================================================\n",
      "Starting Episode 4 from State 3...\n",
      "Time Step  Action     State      Reward     Done      \n",
      "--------------------------------------------------\n",
      "1          LEFT       2          0          0         \n",
      "2          LEFT       1          0          0         \n",
      "3          RIGHT      2          0          0         \n",
      "4          LEFT       1          0          0         \n",
      "5          RIGHT      2          0          0         \n",
      "6          LEFT       1          0          0         \n",
      "7          RIGHT      2          0          0         \n",
      "8          RIGHT      3          0          0         \n",
      "9          LEFT       2          0          0         \n",
      "10         LEFT       1          0          0         \n",
      "11         RIGHT      2          0          0         \n",
      "12         LEFT       1          0          0         \n",
      "13         RIGHT      2          0          0         \n",
      "14         LEFT       1          0          0         \n",
      "15         LEFT       0          0          1         \n",
      "Failed to reach State 6.\n",
      "==================================================\n",
      "Starting Episode 5 from State 3...\n",
      "Time Step  Action     State      Reward     Done      \n",
      "--------------------------------------------------\n",
      "1          LEFT       2          0          0         \n",
      "2          RIGHT      3          0          0         \n",
      "3          LEFT       2          0          0         \n",
      "4          LEFT       1          0          0         \n",
      "5          LEFT       0          0          1         \n",
      "Failed to reach State 6.\n",
      "==================================================\n",
      "Starting Episode 6 from State 3...\n",
      "Time Step  Action     State      Reward     Done      \n",
      "--------------------------------------------------\n",
      "1          LEFT       2          0          0         \n",
      "2          RIGHT      3          0          0         \n",
      "3          LEFT       2          0          0         \n",
      "4          RIGHT      3          0          0         \n",
      "5          LEFT       2          0          0         \n",
      "6          RIGHT      3          0          0         \n",
      "7          LEFT       2          0          0         \n",
      "8          RIGHT      3          0          0         \n",
      "9          LEFT       2          0          0         \n",
      "10         RIGHT      3          0          0         \n",
      "11         RIGHT      4          0          0         \n",
      "12         LEFT       3          0          0         \n",
      "13         LEFT       2          0          0         \n",
      "14         LEFT       1          0          0         \n",
      "15         LEFT       0          0          1         \n",
      "Failed to reach State 6.\n",
      "==================================================\n",
      "Starting Episode 7 from State 3...\n",
      "Time Step  Action     State      Reward     Done      \n",
      "--------------------------------------------------\n",
      "1          LEFT       2          0          0         \n",
      "2          RIGHT      3          0          0         \n",
      "3          LEFT       2          0          0         \n",
      "4          RIGHT      3          0          0         \n",
      "5          LEFT       2          0          0         \n",
      "6          RIGHT      3          0          0         \n",
      "7          LEFT       2          0          0         \n",
      "8          LEFT       1          0          0         \n",
      "9          RIGHT      2          0          0         \n",
      "10         RIGHT      3          0          0         \n",
      "11         LEFT       2          0          0         \n",
      "12         LEFT       1          0          0         \n",
      "13         LEFT       0          0          1         \n",
      "Failed to reach State 6.\n",
      "==================================================\n",
      "Starting Episode 8 from State 3...\n",
      "Time Step  Action     State      Reward     Done      \n",
      "--------------------------------------------------\n",
      "1          LEFT       2          0          0         \n",
      "2          RIGHT      3          0          0         \n",
      "3          RIGHT      4          0          0         \n",
      "4          LEFT       3          0          0         \n",
      "5          LEFT       2          0          0         \n",
      "6          LEFT       1          0          0         \n",
      "7          RIGHT      2          0          0         \n",
      "8          RIGHT      3          0          0         \n",
      "9          LEFT       2          0          0         \n",
      "10         RIGHT      3          0          0         \n",
      "11         LEFT       2          0          0         \n",
      "12         LEFT       1          0          0         \n",
      "13         LEFT       0          0          1         \n",
      "Failed to reach State 6.\n",
      "==================================================\n",
      "Starting Episode 9 from State 3...\n",
      "Time Step  Action     State      Reward     Done      \n",
      "--------------------------------------------------\n",
      "1          RIGHT      4          0          0         \n",
      "2          LEFT       3          0          0         \n",
      "3          RIGHT      4          0          0         \n",
      "4          LEFT       3          0          0         \n",
      "5          LEFT       2          0          0         \n",
      "6          LEFT       1          0          0         \n",
      "7          LEFT       0          0          1         \n",
      "Failed to reach State 6.\n",
      "==================================================\n",
      "Starting Episode 10 from State 3...\n",
      "Time Step  Action     State      Reward     Done      \n",
      "--------------------------------------------------\n",
      "1          LEFT       2          0          0         \n",
      "2          RIGHT      3          0          0         \n",
      "3          LEFT       2          0          0         \n",
      "4          RIGHT      3          0          0         \n",
      "5          LEFT       2          0          0         \n",
      "6          RIGHT      3          0          0         \n",
      "7          LEFT       2          0          0         \n",
      "8          LEFT       1          0          0         \n",
      "9          LEFT       0          0          1         \n",
      "Failed to reach State 6.\n",
      "==================================================\n",
      "Success Rate: 10.00%\n"
     ]
    }
   ],
   "source": [
    "success_rate_policy_b = run_multiple_episodes(env_rw, policy_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0472246-6f9c-4c77-98d8-9fb49e45ce93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_zero(env, policy, alpha=0.01, gamma=1.0, episodes=8000):\n",
    "    V = np.zeros(7)  # Value function initialized to zero for all states    \n",
    "    for episode in tqdm(range(episodes), desc=\"Episodes\"):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        # episode_reward = 0\n",
    "        while not done:\n",
    "            action = np.random.choice([0, 1], p=policy[state])\n",
    "            next_state, reward, done = env.step(action)\n",
    "            V[state] += alpha * (reward + gamma * V[next_state] - V[state])\n",
    "            state = next_state\n",
    "            # episode_reward += reward \n",
    "        # if episode % 10 == 0:\n",
    "        #     print(f\"Episode:{episode}, Reward:{episode_reward}\")\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e39024dc-0e9e-4282-b20f-55f43d00f052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1667, 0.3333, 0.5   , 0.6667, 0.8333])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VF_true = np.arange(1,6)/6\n",
    "VF_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "203d02b0-95a2-408e-84ba-36c0ff83f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, V_true, value_function_estimator, num_runs=10):\n",
    "    rms_errors = []\n",
    "    \n",
    "    for _ in range(num_runs):\n",
    "        # Use the provided value function estimator\n",
    "        value_function = value_function_estimator(env, policy)\n",
    "        print(\"Estimated Value Function (excluding boundaries):\", value_function[1:-1])\n",
    "        \n",
    "        # Compute RMS error\n",
    "        rms_error = np.sqrt(np.mean((value_function[1:-1] - V_true) ** 2))\n",
    "        print(f\"RMS Error: {rms_error}\")\n",
    "        \n",
    "        rms_errors.append(rms_error)\n",
    "    \n",
    "    return rms_errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b10708b3-46ca-4a50-b260-2ac0ab80b0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8000/8000 [00:08<00:00, 956.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function (excluding boundaries): [0.1494 0.3049 0.4655 0.6481 0.8102]\n",
      "RMS Error: 0.025220523424311532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8000/8000 [00:08<00:00, 991.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function (excluding boundaries): [0.1553 0.2892 0.4564 0.6333 0.8138]\n",
      "RMS Error: 0.03307417916416644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8000/8000 [00:07<00:00, 1001.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function (excluding boundaries): [0.1899 0.3863 0.5582 0.711  0.867 ]\n",
      "RMS Error: 0.044346682309311644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8000/8000 [00:07<00:00, 1032.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function (excluding boundaries): [0.1688 0.3309 0.4928 0.6652 0.8202]\n",
      "RMS Error: 0.006894301622267936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8000/8000 [00:08<00:00, 978.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function (excluding boundaries): [0.146  0.2951 0.5071 0.665  0.8392]\n",
      "RMS Error: 0.01986132526461321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8000/8000 [00:07<00:00, 1025.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function (excluding boundaries): [0.1653 0.3098 0.5011 0.6809 0.8177]\n",
      "RMS Error: 0.014165955572612073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8000/8000 [00:08<00:00, 978.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function (excluding boundaries): [0.1665 0.3467 0.5017 0.6662 0.8289]\n",
      "RMS Error: 0.0063562500930327065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8000/8000 [00:07<00:00, 1021.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function (excluding boundaries): [0.1641 0.3509 0.5275 0.6876 0.8356]\n",
      "RMS Error: 0.01738861827080628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8000/8000 [00:08<00:00, 956.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function (excluding boundaries): [0.174  0.3547 0.5219 0.7039 0.8561]\n",
      "RMS Error: 0.024059583489499825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8000/8000 [00:08<00:00, 985.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function (excluding boundaries): [0.1781 0.3489 0.5067 0.6683 0.838 ]\n",
      "RMS Error: 0.009414182117313048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rms_errors_policy_pi = evaluate_policy(env_rw, policy_pi, VF_true, td_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f019c335-6037-44f7-802b-6106f309cdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaptive learning rate decay\n",
    "def td_zero_adaptive(env, policy, alpha_init=0.15, alpha_min=0.01, decay_rate=0.9, gamma=1.0, episodes=2000):\n",
    "    V = np.zeros(7)  # Value function initialized to zero for all states  \n",
    "    alpha = alpha_init\n",
    "    \n",
    "    for episode in tqdm(range(episodes), desc=\"Episodes\"):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        old_V_value = np.copy(V)\n",
    "        \n",
    "        while not done:\n",
    "            action = np.random.choice([0, 1], p=policy[state])\n",
    "            next_state, reward, done = env.step(action)\n",
    "            V[state] += alpha * (reward + gamma * V[next_state] - V[state])\n",
    "            state = next_state\n",
    "        \n",
    "        # Calculate average change in value function\n",
    "        avg_V_change = np.mean(np.abs(V - old_V_value))\n",
    "        \n",
    "        # Update alpha if the change is below the threshold\n",
    "        if avg_V_change < 0.05:\n",
    "            alpha = max(alpha * decay_rate, alpha_min)\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ddf29d6-24af-4bef-aca9-46286a5ed745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:01<00:00, 1033.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function (excluding boundaries): [0.1493 0.3115 0.4798 0.6667 0.8535]\n",
      "RMS Error: 0.017853915197372932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:02<00:00, 965.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function (excluding boundaries): [0.1634 0.3687 0.5368 0.6928 0.8484]\n",
      "RMS Error: 0.02654510470078203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:02<00:00, 980.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function (excluding boundaries): [0.1617 0.3292 0.5029 0.6822 0.8504]\n",
      "RMS Error: 0.010821552008813644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:02<00:00, 895.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function (excluding boundaries): [0.1637 0.3457 0.5234 0.6976 0.845 ]\n",
      "RMS Error: 0.0189815055501554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:02<00:00, 920.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function (excluding boundaries): [0.1819 0.3457 0.4999 0.6832 0.8291]\n",
      "RMS Error: 0.011616832991208198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:02<00:00, 894.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function (excluding boundaries): [0.1727 0.3426 0.504  0.6721 0.8373]\n",
      "RMS Error: 0.006058334901355892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:02<00:00, 941.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function (excluding boundaries): [0.1747 0.3392 0.5029 0.6744 0.842 ]\n",
      "RMS Error: 0.006952535139823722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:02<00:00, 924.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function (excluding boundaries): [0.1349 0.3143 0.4931 0.649  0.8298]\n",
      "RMS Error: 0.01867912696757387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:02<00:00, 902.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function (excluding boundaries): [0.1667 0.3557 0.525  0.7042 0.8391]\n",
      "RMS Error: 0.022665135948945314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:02<00:00, 928.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function (excluding boundaries): [0.1322 0.2784 0.4553 0.644  0.8451]\n",
      "RMS Error: 0.03702779840527947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rms_errors_policy_pi = evaluate_policy(env_rw, policy_pi, VF_true, td_zero_adaptive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d9ee43-2b2c-44bc-9331-9d755fa2f6e6",
   "metadata": {},
   "source": [
    "### Exercise 6.7 \n",
    "Design an off-policy version of the $TD(0)$ update that can be used with arbitrary target policy $\\pi$ and covering behavior policy $b$, using at each step $t$ the importance sampling ratio $\\rho_{t:t}$\n",
    "\n",
    "1. **Importance Sampling Ratio**:\n",
    "   The importance sampling ratio at step $ t $ is defined as:\n",
    "   $$\n",
    "   \\rho_{t:t} = \\frac{\\pi(A_t | S_t)}{b(A_t | S_t)}\n",
    "   $$\n",
    "   This ratio compares the probability of the action taken under the target policy to the probability of the same action under the behavior policy.\n",
    "\n",
    "2. **Off-Policy Update Rule**:\n",
    "   The off-policy $ TD(0) $ update can then be defined as:\n",
    "   $$\n",
    "   V(S_t) \\leftarrow V(S_t) + \\alpha \\rho_{t:t} \\left( R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\right)\n",
    "   $$\n",
    "   where:\n",
    "   - $ \\alpha $ is the step size (learning rate).\n",
    "   - $ \\gamma $ is the discount factor.\n",
    "   - $ S_{t+1} $ is the state resulting from taking action $ A_t $ in state $ S_t $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2975d7f8-86f9-4c5b-816f-6bddc16474f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_policy_td_zero(env, beh_policy, alpha=0.01, gamma=1.0, episodes=2000):\n",
    "    # Initialize value function to zero for all states\n",
    "    V = np.zeros(len(env.states))\n",
    "    pi_prob = 0.5  # Probability for target policy Ï€ (uniform)\n",
    "\n",
    "    for episode in tqdm(range(episodes), desc=\"Episodes\"):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = np.random.choice([0, 1], p=beh_policy[state])\n",
    "            next_state, reward, done = env.step(action)\n",
    "\n",
    "            if not done:\n",
    "                rho = pi_prob / beh_policy[state, action]\n",
    "                td_target = reward + gamma * V[next_state]\n",
    "                V[state] += alpha * rho * (td_target - V[state])\n",
    "                # Debugging output\n",
    "                # print(f\"Episode: {episode}, State: {state}, Action: {action}, Next State: {next_state}, Reward: {reward}, V: {V}\")\n",
    "            else:\n",
    "                # Update the last state when reaching terminal state\n",
    "                rho = pi_prob / beh_policy[state, action]\n",
    "                V[state] += alpha * rho * (reward - V[state])  # Update value for the terminal state as well\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1955102f-2a64-4932-ba47-2cd303b69a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:02<00:00, 903.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function (excluding boundaries): [0.1542 0.3038 0.4822 0.6662 0.8471]\n",
      "RMS Error: 0.017494503321701674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:02<00:00, 830.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function (excluding boundaries): [0.1712 0.3189 0.5037 0.6654 0.837 ]\n",
      "RMS Error: 0.007191204199005797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:02<00:00, 799.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function (excluding boundaries): [0.1621 0.3156 0.4636 0.6404 0.8274]\n",
      "RMS Error: 0.021840395094186487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:02<00:00, 825.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function (excluding boundaries): [0.1606 0.3314 0.4879 0.6404 0.7941]\n",
      "RMS Error: 0.02200026673343897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:02<00:00, 901.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function (excluding boundaries): [0.143  0.3196 0.4867 0.6481 0.8272]\n",
      "RMS Error: 0.01617387581657938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:02<00:00, 864.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function (excluding boundaries): [0.1659 0.3343 0.5141 0.6619 0.8307]\n",
      "RMS Error: 0.006778855023927686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:02<00:00, 814.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function (excluding boundaries): [0.1527 0.3442 0.5034 0.6566 0.8348]\n",
      "RMS Error: 0.009250422148350423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:02<00:00, 914.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function (excluding boundaries): [0.142  0.3273 0.5001 0.6788 0.8278]\n",
      "RMS Error: 0.012819664446075157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:02<00:00, 853.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function (excluding boundaries): [0.1582 0.3405 0.4961 0.6843 0.8416]\n",
      "RMS Error: 0.010186446142345174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:02<00:00, 868.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Value Function (excluding boundaries): [0.1949 0.3497 0.5081 0.6438 0.802 ]\n",
      "RMS Error: 0.022964020406212607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rms_errors_off_policy = evaluate_policy(env_rw, policy_b, VF_true, off_policy_td_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a98162-695b-4bef-a52e-aedb408692ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc5222c2-df22-43f8-b5e4-159b117dd3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:07<00:00, 1379.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated State-Value Function:\n",
      "V(0) = 0.0000\n",
      "V(1) = 0.1447\n",
      "V(2) = 0.3197\n",
      "V(3) = 0.4952\n",
      "V(4) = 0.6743\n",
      "V(5) = 0.8454\n",
      "V(6) = 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "num_states = 7  # Total number of states\n",
    "alpha = 0.01     # Learning rate\n",
    "gamma = 1.0     # Discount factor\n",
    "num_episodes = 10000\n",
    "\n",
    "# Initialize state-value function\n",
    "V = np.zeros(num_states)  # Initialize state-value function for all 7 states\n",
    "V[1:6] = 0.5\n",
    "# TD(0) algorithm\n",
    "for episode in tqdm(range(num_episodes), desc=\"Episodes\"):\n",
    "    state = 3  # Start in the middle state\n",
    "    while state != 0 and state != 6:  # Run until a terminal state is reached\n",
    "        # Choose next state: move left or right\n",
    "        next_state = state + np.random.choice([-1, 1])\n",
    "        \n",
    "        # Ensure next_state is within bounds (0 to 6)\n",
    "        next_state = max(0, min(next_state, 6))\n",
    "        \n",
    "        # Assign reward based on whether next_state is terminal\n",
    "        reward = 1 if next_state == 6 else 0\n",
    "        \n",
    "        # TD(0) update\n",
    "        V[state] += alpha * (reward + gamma * V[next_state] - V[state])\n",
    "        \n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "# Print the estimated state-value function\n",
    "print(\"Estimated State-Value Function:\")\n",
    "for state in range(num_states):\n",
    "    print(f\"V({state}) = {V[state]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a353cf1d-40e7-47e8-9a78-3a2adaf8a66d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
